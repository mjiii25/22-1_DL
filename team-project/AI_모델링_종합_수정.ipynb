{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI 모델링 종합_",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjiii25/22-1_DLAI/blob/main/team-project/AI_%EB%AA%A8%EB%8D%B8%EB%A7%81_%EC%A2%85%ED%95%A9_%EC%88%98%EC%A0%95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 데이터 불러오기"
      ],
      "metadata": {
        "id": "Hetd7rFnPWRI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtSTejoENxoU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data path\n",
        "dpath = \"/content/drive/MyDrive/\""
      ],
      "metadata": {
        "id": "WqIdQSrvOD_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import data\n",
        "import pandas as pd\n",
        "train_features = pd.read_csv(dpath+\"training_set_features.csv\")\n",
        "train_labels = pd.read_csv(dpath+\"training_set_labels.csv\")\n",
        "test_features = pd.read_csv(dpath+\"test_set_features.csv\")"
      ],
      "metadata": {
        "id": "-3UumU13O6OJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 딥러닝 모형"
      ],
      "metadata": {
        "id": "ahNFE6yeVG4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_h1n1_ver1 = pd.read_csv('/content/drive/MyDrive/22-1_DLAI_Team_Project/train_h1n1_ver1.csv')\n",
        "train_seasonal_ver1 = pd.read_csv('/content/drive/MyDrive/22-1_DLAI_Team_Project/train_seasonal_ver1.csv')"
      ],
      "metadata": {
        "id": "Kb69hj9RcI8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_h1n1_ver1.shape)\n",
        "train_h1n1_ver1.head()"
      ],
      "metadata": {
        "id": "1CWSrjETcg3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "feature = train_h1n1_ver1.iloc[:,1:-1]\n",
        "target = train_h1n1_ver1['h1n1_vaccine']\n",
        "\n",
        "x_train_h1n1_ver1, x_valid_h1n1_ver1, y_train_h1n1_ver1, y_valid_h1n1_ver1 = train_test_split(feature, target, test_size=0.2,shuffle=True,\n",
        "                                                                                              stratify=target,  random_state=1234) "
      ],
      "metadata": {
        "id": "tEpKxPgUcpAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## loss: 0.4388 - accuracy: 0.8098 - val_loss: 0.4563 - val_accuracy: 0.8059\n",
        "## loss: 0.4390 - accuracy: 0.8098 - val_loss: 0.4555 - val_accuracy: 0.8004\n",
        "## loss: 0.4378 - accuracy: 0.8088 - val_loss: 0.4556 - val_accuracy: 0.8014\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras import regularizers\n",
        "\n",
        "\n",
        "# building model\n",
        "\n",
        "model_h1n1_ver1 = keras.Sequential([\n",
        "                                    layers.Dense(16, activation = \"relu\"),\n",
        "                                    layers.Dense(16, activation = \"relu\"),\n",
        "                                    layers.Dense(1, activation = \"sigmoid\")\n",
        "])\n",
        "\n",
        "\n",
        "# compiling model\n",
        "\n",
        "model_h1n1_ver1.compile(optimizer = \"rmsprop\",\n",
        "                        loss = \"binary_crossentropy\",\n",
        "                        metrics = [\"accuracy\"])\n",
        "\n",
        "\n",
        "\n",
        "# fitting model\n",
        "\n",
        "history_h1n1_ver1 = model_h1n1_ver1.fit(x_train_h1n1_ver1,\n",
        "                                        y_train_h1n1_ver1,\n",
        "                                        epochs = 20,\n",
        "                                        batch_size = 512,\n",
        "                                        validation_data = (x_valid_h1n1_ver1, y_valid_h1n1_ver1))\n"
      ],
      "metadata": {
        "id": "EMJKkJl3dzeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## loss: 0.4008 - accuracy: 0.8234 - val_loss: 0.4623 - val_accuracy: 0.8111\n",
        "## loss: 0.4053 - accuracy: 0.8209 - val_loss: 0.4523 - val_accuracy: 0.7988\n",
        "## loss: 0.4026 - accuracy: 0.8260 - val_loss: 0.4506 - val_accuracy: 0.8095\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras import regularizers\n",
        "\n",
        "\n",
        "# building model\n",
        "\n",
        "model_h1n1_ver1 = keras.Sequential([\n",
        "                                    layers.Dense(256, activation = \"relu\"),\n",
        "                                    layers.Dense(128, activation = \"relu\"),\n",
        "                                    layers.Dense(64, activation = \"relu\"),\n",
        "                                    layers.Dense(64, activation = \"relu\"),\n",
        "                                    layers.Dense(16, activation = \"relu\"),\n",
        "                                    layers.Dense(16, activation = \"relu\"),\n",
        "                                    layers.Dense(1, activation = \"sigmoid\")\n",
        "])\n",
        "\n",
        "\n",
        "# compiling model\n",
        "\n",
        "model_h1n1_ver1.compile(optimizer = \"rmsprop\",\n",
        "                        loss = \"binary_crossentropy\",\n",
        "                        metrics = [\"accuracy\"])\n",
        "\n",
        "\n",
        "\n",
        "# fitting model\n",
        "\n",
        "history_h1n1_ver1 = model_h1n1_ver1.fit(x_train_h1n1_ver1,\n",
        "                                        y_train_h1n1_ver1,\n",
        "                                        epochs = 20,\n",
        "                                        batch_size = 512,\n",
        "                                        validation_data = (x_valid_h1n1_ver1, y_valid_h1n1_ver1))\n"
      ],
      "metadata": {
        "id": "qzbU91aIis0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## loss: 0.4237 - accuracy: 0.8141 - val_loss: 0.4461 - val_accuracy: 0.8111\n",
        "## loss: 0.4187 - accuracy: 0.8156 - val_loss: 0.4599 - val_accuracy: 0.8087\n",
        "## loss: 0.4244 - accuracy: 0.8132 - val_loss: 0.4421 - val_accuracy: 0.8111\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras import regularizers\n",
        "\n",
        "\n",
        "# building model\n",
        "\n",
        "model_h1n1_ver1 = keras.Sequential([\n",
        "                                    layers.Dense(256, activation = \"relu\"),\n",
        "                                    layers.Dense(128, activation = \"relu\"),\n",
        "                                    layers.Dropout(0.5),\n",
        "                                    layers.Dense(64, activation = \"relu\"),\n",
        "                                    layers.Dense(64, activation = \"relu\"),\n",
        "                                    #layers.Dropout(0.5),\n",
        "                                    layers.Dense(16, activation = \"relu\"),\n",
        "                                    layers.Dense(16, activation = \"relu\"),\n",
        "                                    layers.Dense(1, activation = \"sigmoid\")\n",
        "])\n",
        "\n",
        "\n",
        "# compiling model\n",
        "\n",
        "model_h1n1_ver1.compile(optimizer = \"rmsprop\",\n",
        "                        loss = \"binary_crossentropy\",\n",
        "                        metrics = [\"accuracy\"])\n",
        "\n",
        "\n",
        "\n",
        "# fitting model\n",
        "\n",
        "history_h1n1_ver1 = model_h1n1_ver1.fit(x_train_h1n1_ver1,\n",
        "                                        y_train_h1n1_ver1,\n",
        "                                        epochs = 20,\n",
        "                                        batch_size = 512,\n",
        "                                        validation_data = (x_valid_h1n1_ver1, y_valid_h1n1_ver1))\n"
      ],
      "metadata": {
        "id": "6Fw16LJBlMru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## loss: 0.4282 - accuracy: 0.8128 - val_loss: 0.4426 - val_accuracy: 0.8111\n",
        "## loss: 0.4268 - accuracy: 0.8115 - val_loss: 0.4554 - val_accuracy: 0.7942\n",
        "## loss: 0.4280 - accuracy: 0.8114 - val_loss: 0.4430 - val_accuracy: 0.8115\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras import regularizers\n",
        "\n",
        "\n",
        "# building model\n",
        "\n",
        "model_h1n1_ver1 = keras.Sequential([\n",
        "                                    layers.Dense(256, activation = \"relu\"),\n",
        "                                    layers.Dense(128, activation = \"relu\"),\n",
        "                                    layers.Dropout(0.5),\n",
        "                                    layers.Dense(64, activation = \"relu\"),\n",
        "                                    layers.Dense(64, activation = \"relu\"),\n",
        "                                    layers.Dropout(0.5),\n",
        "                                    layers.Dense(16, activation = \"relu\"),\n",
        "                                    layers.Dense(16, activation = \"relu\"),\n",
        "                                    layers.Dense(1, activation = \"sigmoid\")\n",
        "])\n",
        "\n",
        "\n",
        "# compiling model\n",
        "\n",
        "model_h1n1_ver1.compile(optimizer = \"rmsprop\",\n",
        "                        loss = \"binary_crossentropy\",\n",
        "                        metrics = [\"accuracy\"])\n",
        "\n",
        "\n",
        "\n",
        "# fitting model\n",
        "\n",
        "history_h1n1_ver1 = model_h1n1_ver1.fit(x_train_h1n1_ver1,\n",
        "                                        y_train_h1n1_ver1,\n",
        "                                        epochs = 20,\n",
        "                                        batch_size = 512,\n",
        "                                        validation_data = (x_valid_h1n1_ver1, y_valid_h1n1_ver1))\n"
      ],
      "metadata": {
        "id": "VNCri7talqUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## loss: 0.4312 - accuracy: 0.8182 - val_loss: 0.4567 - val_accuracy: 0.8053\n",
        "## loss: 0.4305 - accuracy: 0.8179 - val_loss: 0.4690 - val_accuracy: 0.7894\n",
        "## loss: 0.4323 - accuracy: 0.8160 - val_loss: 0.4606 - val_accuracy: 0.8107\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras import regularizers\n",
        "\n",
        "\n",
        "# building model\n",
        "\n",
        "model_h1n1_ver1 = keras.Sequential([\n",
        "                                    layers.Dense(256, activation = \"relu\"),\n",
        "                                    layers.Dense(128, activation = \"relu\"),\n",
        "                                    layers.Dense(64, activation = \"relu\", kernel_regularizer = regularizers.l1(0.001)),\n",
        "                                    layers.Dense(64, activation = \"relu\"),\n",
        "                                    layers.Dense(16, activation = \"relu\"),\n",
        "                                    layers.Dense(16, activation = \"relu\"),\n",
        "                                    layers.Dense(1, activation = \"sigmoid\")\n",
        "])\n",
        "\n",
        "\n",
        "# compiling model\n",
        "\n",
        "model_h1n1_ver1.compile(optimizer = \"rmsprop\",\n",
        "                        loss = \"binary_crossentropy\",\n",
        "                        metrics = [\"accuracy\"])\n",
        "\n",
        "\n",
        "\n",
        "# fitting model\n",
        "\n",
        "history_h1n1_ver1 = model_h1n1_ver1.fit(x_train_h1n1_ver1,\n",
        "                                        y_train_h1n1_ver1,\n",
        "                                        epochs = 20,\n",
        "                                        batch_size = 512,\n",
        "                                        validation_data = (x_valid_h1n1_ver1, y_valid_h1n1_ver1))\n"
      ],
      "metadata": {
        "id": "brxMgts2l6f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss result\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict_h1n1_ver1 = history_h1n1_ver1.history\n",
        "\n",
        "loss_values_h1n1_ver1 = history_dict_h1n1_ver1[\"loss\"]\n",
        "val_loss_values_h1n1_ver1 = history_dict_h1n1_ver1[\"val_loss\"]\n",
        "epochs = range(1, len(loss_values_h1n1_ver1) + 1)\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(epochs, loss_values_h1n1_ver1, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss_values_h1n1_ver1, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss of h1n1 vaccine_ver1\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# accuracy result\n",
        "\n",
        "acc_h1n1_ver1 = history_dict_h1n1_ver1[\"accuracy\"]\n",
        "val_acc_h1n1_ver1 = history_dict_h1n1_ver1[\"val_accuracy\"]\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(epochs, acc_h1n1_ver1, \"bo\", label=\"Training acc\")\n",
        "plt.plot(epochs, val_acc_h1n1_ver1, \"b\", label=\"Validation acc\")\n",
        "plt.title(\"Training and validation accuracy of h1n1 vaccine_ver1\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GEXCr7qIeV-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "|number|layer|train loss|train accuracy|validation loss|validation accuracy|\n",
        "|------|-----|----------|--------------|---------------|-------------------|\n",
        "|1     |layers.Dense(16, activation = \"relu\"),</br>layers.Dense(16, activation = \"relu\"),</br>layers.Dense(1, activation = \"sigmoid\")|ㅇ|ㅇ|ㅇ|ㅇ|\n",
        "|2     |layers.Dense(256, activation = \"relu\"),</br>layers.Dense(128, activation = \"relu\"),</br>layers.Dense(64, activation = \"relu\"),</br>layers.Dense(64, activation = \"relu\"),</br>layers.Dense(16, activation = \"relu\"),</br>layers.Dense(16, activation = \"relu\"),</br>layers.Dense(1, activation = \"sigmoid\")|ㅇ|ㅇ|ㅇ|ㅇ|\n",
        "|"
      ],
      "metadata": {
        "id": "Rv6riW7-gcNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## loss: 0.4176 - accuracy: 0.8190 - val_loss: 0.4562 - val_accuracy: 0.7974\n",
        "## loss: 0.4237 - accuracy: 0.8216 - val_loss: 0.4572 - val_accuracy: 0.8075\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras import regularizers\n",
        "\n",
        "model_h1n1 = keras.Sequential([\n",
        "                               layers.Dense(256, activation=\"relu\"),\n",
        "                               layers.Dense(256, activation=\"relu\"),\n",
        "                               layers.Dense(128, activation=\"relu\"),\n",
        "                               layers.Dropout(0.5),\n",
        "                               layers.Dense(128, activation=\"relu\"),\n",
        "                               layers.Dense(64, activation=\"relu\", kernel_regularizer = regularizers.l2(0.001)),\n",
        "                               layers.Dropout(0.5),\n",
        "                               layers.Dense(16, activation=\"relu\"),\n",
        "                               layers.Dense(8, activation=\"relu\", kernel_regularizer = regularizers.l2(0.001)),\n",
        "                               layers.Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "metadata": {
        "id": "30bati9tiMHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy result\n",
        "\n",
        "plt.clf()\n",
        "acc_h1n1 = history_dict_h1n1[\"accuracy\"]\n",
        "val_acc_h1n1 = history_dict_h1n1[\"val_accuracy\"]\n",
        "plt.plot(epochs, acc_h1n1, \"bo\", label=\"Training acc\")\n",
        "plt.plot(epochs, val_acc_h1n1, \"b\", label=\"Validation acc\")\n",
        "plt.title(\"Training and validation accuracy of h1n1 vaccine\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CvPruFnTealc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## [참고]\n",
        "## 모델 성능을 올리기 위한 방법 중 하나 : 배치 정규화\n",
        "## 값이 활성화 함수를 통과하기 전에 가중의 변화를 줄이는 것\n",
        "## https://everyday-deeplearning.tistory.com/entry/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%9C%BC%EB%A1%9C-%EB%94%A5%EB%9F%AC%EB%8B%9D%ED%95%98%EA%B8%B0-%EC%8B%A0%EA%B2%BD%EB%A7%9D-%ED%95%99%EC%8A%B5%EC%9D%98-%ED%9A%A8%EC%9C%A8%EA%B3%BC-%EC%A0%95%ED%99%95%EB%8F%84-%EC%98%AC%EB%A6%AC%EA%B8%B0\n",
        "\n",
        "## layers에서 사용할 수 없는 메소드여서 적용은 못 해봤어요 ㅠㅅㅠ\n",
        "\n",
        "\n",
        "class BatchNormalization :\n",
        "\n",
        "  def __init__(self, gamma, beta, momentum = 0.9, running_mean = None, running_var = None) :\n",
        "    self.gamma = gamma\n",
        "    self. beta = beta\n",
        "    self.momentum = momentum\n",
        "    self.input_shape = None\n",
        "\n",
        "    # 테스트에서 사용할 평균과 분산\n",
        "    self.running_mean = running_mean\n",
        "    self.running_var = running_var\n",
        "\n",
        "    # backward시 사용할 중간 데이터\n",
        "    self.batch_size = None\n",
        "    self.xc = None\n",
        "    self.std = None\n",
        "    self.dgamma = None\n",
        "    self.dbeta = None\n",
        "\n",
        "    def forward(self, x, train_flg = True) :\n",
        "      self.input_shape = x.shape\n",
        "      if x.ndim != 2 :\n",
        "        N, C, H, W = x.shape\n",
        "        x = x.reshape(N, -1)\n",
        "\n",
        "      out = self.__forward(x, train_flg)\n",
        "\n",
        "      return out.reshape(*self.input_shape)\n",
        "\n",
        "    def __forward(self, x, train_flg) :\n",
        "      if self.running_mean is None :\n",
        "        N, D = x.shape\n",
        "        self.running_mean = np.zeros(D)\n",
        "        self.runnng_var = np.zeros(D)\n",
        "\n",
        "      if train_flg :\n",
        "        mu = x.mean(axis = 0)\n",
        "        xc = x - mu\n",
        "        var = np.mean(xc * 2, axis = 0)\n",
        "        std = np.sqrt(var + 10e-7)\n",
        "        xn = xc / std\n",
        "\n",
        "        self.batch_size = x.shape[0]\n",
        "        self.xc = xc\n",
        "        self.xn = xn\n",
        "        self.std = std\n",
        "        self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mu\n",
        "        self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\n",
        "      else :\n",
        "        xc = x - self.running_mean\n",
        "        xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
        "\n",
        "      out = self.gamma * xn + self. beta\n",
        "      return out\n",
        "\n",
        "    def backward(self, dout) :\n",
        "      if dout.ndim != 2 :\n",
        "        N, C, H, W = dout.shape\n",
        "        dout = dout.reshape(N,-1)\n",
        "\n",
        "      dx = self.__backward(dout)\n",
        "      dx = dx.reshape(*self.input_shape)\n",
        "      return dx\n",
        "\n",
        "    def __backward(self, dout) :\n",
        "      dbeta = dout.sum(axis = 0)\n",
        "      dgamma = np.sum(self.xn * dout, axis = 0)\n",
        "      dxn = self.gamma * dout\n",
        "      dxc = dxn / self.std\n",
        "      dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis = 0)\n",
        "      dvar = 0.5 * dstd / self.std\n",
        "      dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
        "      dmu = np.sum(dxc, axis = 0)\n",
        "      dx = dxc - dmu / self. batch_size\n",
        "\n",
        "      self.dgamma = dgamma\n",
        "      self.dbeta = dbeta\n",
        "\n",
        "      return dx\n"
      ],
      "metadata": {
        "id": "7GWhEmm1nCgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### seasonal 모델"
      ],
      "metadata": {
        "id": "ifwlLAgFND2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## age_group\n",
        "\n",
        "train_seasonal.loc[train_seasonal['age_group'] == '18 - 34 Years', 'age_group'] = 1\n",
        "train_seasonal.loc[train_seasonal['age_group'] == '35 - 44 Years', 'age_group'] = 2\n",
        "train_seasonal.loc[train_seasonal['age_group'] == '45 - 54 Years', 'age_group'] = 3\n",
        "train_seasonal.loc[train_seasonal['age_group'] == '55 - 64 Years', 'age_group'] = 4\n",
        "train_seasonal.loc[train_seasonal['age_group'] == '65+ Years', 'age_group'] = 5\n",
        "\n",
        "\n",
        "## race\n",
        "\n",
        "train_seasonal.loc[train_seasonal['race'] == 'White', 'race'] = 1\n",
        "train_seasonal.loc[train_seasonal['race'] == 'Black', 'race'] = 2\n",
        "train_seasonal.loc[train_seasonal['race'] == 'Other or Multiple', 'race'] = 3\n",
        "train_seasonal.loc[train_seasonal['race'] == 'Hispanic', 'race'] = 4\n",
        "\n",
        "\n",
        "## sex\n",
        "\n",
        "train_seasonal.loc[train_seasonal['sex'] == 'Female', 'sex'] = 1\n",
        "train_seasonal.loc[train_seasonal['sex'] == 'Male', 'sex'] = 2\n",
        "\n",
        "\n",
        "## hhs_geo_region\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(train_seasonal['hhs_geo_region'])\n",
        "\n",
        "print(le.classes_)\n",
        "train_seasonal['hhs_geo_region'] = le.transform(train_seasonal['hhs_geo_region'])\n",
        "\n",
        "\n",
        "## census_msa\n",
        "\n",
        "train_seasonal.loc[train_seasonal['census_msa'] == 'Non-MSA', 'census_msa'] = 1\n",
        "train_seasonal.loc[train_seasonal['census_msa'] == 'MSA, Not Principle  City', 'census_msa'] = 2\n",
        "train_seasonal.loc[train_seasonal['census_msa'] == 'MSA, Principle City', 'census_msa'] = 3\n"
      ],
      "metadata": {
        "id": "vQWag62vekRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 훈련을 위해 데이터타입 변환해주기\n",
        "\n",
        "train_seasonal['age_group'] = train_seasonal['age_group'].astype(np.int64)\n",
        "train_seasonal['race'] = train_seasonal['race'].astype(np.int64)\n",
        "train_seasonal['sex'] = train_seasonal['sex'].astype(np.int64)\n",
        "train_seasonal['census_msa'] = train_seasonal['census_msa'].astype(np.int64)"
      ],
      "metadata": {
        "id": "hoReETlxQPAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "feature = train_seasonal.iloc[:,1:-1]\n",
        "target = train_seasonal['seasonal_vaccine']\n",
        "\n",
        "x_train_seasonal, x_valid_seasonal, y_train_seasonal, y_valid_seasonal = train_test_split(feature, target, test_size=0.2,shuffle=True,\n",
        "                                                                                          stratify=target,  random_state=1234) "
      ],
      "metadata": {
        "id": "HR3XAcFTqy8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델 정의"
      ],
      "metadata": {
        "id": "-b_JY8lFNU2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## loss: 0.5118 - accuracy: 0.7644 - val_loss: 0.5734 - val_accuracy: 0.7325\n",
        "## loss: 0.5112 - accuracy: 0.7603 - val_loss: 0.5583 - val_accuracy: 0.7303\n",
        "\n",
        "## l2 규제 적용\n",
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras import regularizers\n",
        "\n",
        "model_seasonal = keras.Sequential([\n",
        "                               layers.Dense(256, activation=\"relu\"),\n",
        "                               layers.Dense(256, activation=\"relu\"),\n",
        "                               layers.Dense(128, activation=\"relu\"),\n",
        "                               layers.Dropout(0.5),\n",
        "                               layers.Dense(128, activation=\"relu\"),\n",
        "                               layers.Dense(64, activation=\"relu\", kernel_regularizer = regularizers.l2(0.001)),\n",
        "                               layers.Dropout(0.5),\n",
        "                               layers.Dense(16, activation=\"relu\", kernel_regularizer = regularizers.l2(0.001)),\n",
        "                               layers.Dense(8, activation=\"relu\"),\n",
        "                               layers.Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "metadata": {
        "id": "gwrdzeV2seZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_seasonal.compile(optimizer=\"rmsprop\",\n",
        "                       loss=\"binary_crossentropy\",\n",
        "                       metrics=[\"accuracy\"])    "
      ],
      "metadata": {
        "id": "DScwpCgGfOGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_seasonal = model_seasonal.fit(x_train_seasonal,\n",
        "                                      y_train_seasonal,\n",
        "                                      epochs=20,\n",
        "                                      batch_size=512,\n",
        "                                      validation_data=(x_valid_seasonal, y_valid_seasonal))"
      ],
      "metadata": {
        "id": "wyYfzE3pr-Xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss result\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history_dict_seasonal = history_seasonal.history\n",
        "loss_values_seasonal = history_dict_seasonal[\"loss\"]\n",
        "val_loss_values_seasonal = history_dict_seasonal[\"val_loss\"]\n",
        "\n",
        "epochs = range(1, len(loss_values_seasonal) + 1)\n",
        "plt.plot(epochs, loss_values_seasonal, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss_values_seasonal, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss of seasonal vaccine\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kwjto1F6fXTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# accuracy result\n",
        "\n",
        "plt.clf()\n",
        "acc_seasonal = history_dict_seasonal[\"accuracy\"]\n",
        "val_acc_seasonal = history_dict_seasonal[\"val_accuracy\"]\n",
        "plt.plot(epochs, acc_seasonal, \"bo\", label=\"Training acc\")\n",
        "plt.plot(epochs, val_acc_seasonal, \"b\", label=\"Validation acc\")\n",
        "plt.title(\"Training and validation accuracy of seasonal vaccine\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UIWDSCrZfbm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_seasonal.evaluate(x_valid_seasonal, y_valid_seasonal)"
      ],
      "metadata": {
        "id": "3y83b3WJ0GsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) 코드2 ~> 인공신경망 모델"
      ],
      "metadata": {
        "id": "7Ly3SfNJywST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "36Pte4RezGWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_features_data = pd.read_csv(dpath+\"training_set_features.csv\")\n",
        "training_set_labels = pd.read_csv(dpath+\"training_set_labels.csv\")\n",
        "test_features_data = pd.read_csv(dpath+\"test_set_features.csv\")"
      ],
      "metadata": {
        "id": "HNCdybvuzc5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결측치 처리\n",
        "\n",
        "# float types -> mean 값으로 대체\n",
        "training_features_data=training_features_data.fillna(training_features_data.mean())\n",
        "\n",
        "# string types -> null값을 'out-of-category'\n",
        "training_features_data=training_features_data.fillna('out-of-category')"
      ],
      "metadata": {
        "id": "5el-vPRzz81W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_features_data.isna().sum()"
      ],
      "metadata": {
        "id": "tChcK2nR0OlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding categorical features (str -> float)\n",
        "\n",
        "enc = OrdinalEncoder()\n",
        "\n",
        "enc.fit(training_features_data)\n",
        "training_features_data_arr=enc.transform(training_features_data)\n",
        "\n",
        "col_names_list=training_features_data.columns\n",
        "encoded_categorical_df=pd.DataFrame(training_features_data_arr, columns=col_names_list)"
      ],
      "metadata": {
        "id": "GM9BF8w20SD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalization(make all values bet. 0-1)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(encoded_categorical_df)\n",
        "normalized_arr=scaler.transform(encoded_categorical_df)\n",
        "\n",
        "normalized_df=pd.DataFrame(normalized_arr, columns=col_names_list)"
      ],
      "metadata": {
        "id": "EpNknkj_0ga1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if data types are correct or not \n",
        "\n",
        "normalized_df.info()"
      ],
      "metadata": {
        "id": "V1FquagJ0jb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check types of test dataset\n",
        "test_features_data.info()"
      ],
      "metadata": {
        "id": "qYhhvq6G0mt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결측치 처리\n",
        "\n",
        "# float types -> mean 값으로 대체\n",
        "test_features_data=test_features_data.fillna(test_features_data.mean())\n",
        "\n",
        "## string types -> null값을 'out-of-category'\n",
        "test_features_data=test_features_data.fillna('out-of-category')"
      ],
      "metadata": {
        "id": "Hc6vAmRH0p_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_features_data.isna().sum()"
      ],
      "metadata": {
        "id": "puAxXtUY03FN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding categorical features  (str -> float)\n",
        "enc = OrdinalEncoder()\n",
        "enc.fit(test_features_data)\n",
        "test_features_data_arr=enc.transform(test_features_data)\n",
        "\n",
        "col_names_list=test_features_data.columns\n",
        "test_encoded_categorical_df=pd.DataFrame(test_features_data_arr, columns=col_names_list)"
      ],
      "metadata": {
        "id": "jxP6hfGS06hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check data types\n",
        "test_encoded_categorical_df.info()"
      ],
      "metadata": {
        "id": "IdejAEDd0_B5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#normalization(bet. 0-1)\n",
        "\n",
        "#using minmax scaler(look up)\n",
        "test_normalized_arr=scaler.transform(test_encoded_categorical_df)\n",
        "test_normalized_df=pd.DataFrame(test_normalized_arr, columns=col_names_list)"
      ],
      "metadata": {
        "id": "czfETncj1CdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split df to X and Y\n",
        "y = training_set_labels.loc[:, 'seasonal_vaccine'].values\n",
        "X = normalized_df"
      ],
      "metadata": {
        "id": "07mFkM0X1Flb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 80: training set, 20: test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y)\n",
        "\n",
        "# 5 fold CV\n",
        "cv = StratifiedShuffleSplit(n_splits=5, random_state = 42)"
      ],
      "metadata": {
        "id": "qTQbdq5e1Ido"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display test scores and return result string and indexes of false samples\n",
        "def display_test_scores(test, pred):\n",
        "    str_out = \"\"\n",
        "    str_out += (\"TEST SCORES\\n\")\n",
        "    str_out += (\"\\n\")\n",
        "\n",
        "    #print AUC score\n",
        "    auc = roc_auc_score(test, pred)\n",
        "    str_out += (\"AUC: {:.4f}\\n\".format(auc))\n",
        "    str_out += (\"\\n\")\n",
        "    \n",
        "    false_indexes = np.where(test != pred)\n",
        "    return str_out, false_indexes"
      ],
      "metadata": {
        "id": "PdLhFmg31Zyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NN with 1 layer\n",
        "nn_1 = MLPRegressor(tol=1e-5, hidden_layer_sizes=10, random_state=0, solver='adam', activation='relu', max_iter=1000, batch_size=2048)\n",
        "nn_1.fit(X, y)\n",
        "\n",
        "# prediction results\n",
        "y_pred = nn_1.predict(test_normalized_df)"
      ],
      "metadata": {
        "id": "N8l4wSBF1dQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NN with 1 layer\n",
        "nn_2 = MLPRegressor(tol=1e-5, hidden_layer_sizes=10, random_state=0, solver='adam', activation='logistic', max_iter=1000, batch_size=512)\n",
        "nn_2.fit(X, y)\n",
        "\n",
        "# prediction results\n",
        "y_pred_2 = nn_2.predict(test_normalized_df)"
      ],
      "metadata": {
        "id": "JkyCFDsb1gLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(np.logical_or(np.array(y_pred_2) > 1, np.array(y_pred_2) < 0), axis=0)\n",
        "y_pred_2 = 1/(1+np.exp(-y_pred_2))"
      ],
      "metadata": {
        "id": "jSjiD8T81qwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pred_seasonal_vaccine=pd.DataFrame(y_pred_2, columns=['seasonal_vaccine'])\n",
        "df_pred_seasonal_vaccine[\"respondent_id\"] = df_pred_seasonal_vaccine.index\n",
        "\n",
        "df_pred_seasonal_vaccine=df_pred_seasonal_vaccine[['respondent_id', 'seasonal_vaccine']]\n",
        "\n",
        "df_pred_seasonal_vaccine.to_csv('df_seasonal_nn_log.csv', columns=['respondent_id', 'seasonal_vaccine'], \n",
        "                            index=False, sep=',')"
      ],
      "metadata": {
        "id": "q8zwnPME2IJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pred_seasonal_vaccine.head()"
      ],
      "metadata": {
        "id": "D6q32RQG2QKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pred_h1n1 = pd.read_csv(dpath+\"df_h1n1_nn_log_son.csv\",\n",
        "                    sep=',')\n",
        "\n",
        "df_pred_h1n1.head()"
      ],
      "metadata": {
        "id": "IWeHducy2TJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final = df_pred_h1n1.merge(df_pred_seasonal_vaccine, on=\"respondent_id\", how = 'inner')\n",
        "\n",
        "df_final['respondent_id'] = df_final['respondent_id'].astype(int) + 26707"
      ],
      "metadata": {
        "id": "ffu-mmps2tYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.to_csv('df_nn_log.csv', columns=['respondent_id', 'h1n1_vaccine', 'seasonal_vaccine' ], \n",
        "                            index=False, sep=',')"
      ],
      "metadata": {
        "id": "T6R4uVz-4cQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.head()"
      ],
      "metadata": {
        "id": "Vyl2bRGb-1ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aUHtnIfnA9DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 예측하기"
      ],
      "metadata": {
        "id": "bhonNAK5CcUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "SLkwnuicFmj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_h1n1 = pd.read_csv(dpath+\"test_ver1_h1n1.csv\")\n",
        "test_seasonal = pd.read_csv(dpath+\"test_ver1_seasonal.csv\")"
      ],
      "metadata": {
        "id": "L7E9Bl29FePz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### h1n1 예측하기"
      ],
      "metadata": {
        "id": "dIsZGjS8TNbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## h1n1 인코딩하기\n",
        "\n",
        "## age_group\n",
        "\n",
        "test_h1n1.loc[test_h1n1['age_group'] == '18 - 34 Years', 'age_group'] = 1\n",
        "test_h1n1.loc[test_h1n1['age_group'] == '35 - 44 Years', 'age_group'] = 2\n",
        "test_h1n1.loc[test_h1n1['age_group'] == '45 - 54 Years', 'age_group'] = 3\n",
        "test_h1n1.loc[test_h1n1['age_group'] == '55 - 64 Years', 'age_group'] = 4\n",
        "test_h1n1.loc[test_h1n1['age_group'] == '65+ Years', 'age_group'] = 5\n",
        "\n",
        "\n",
        "## race\n",
        "\n",
        "test_h1n1.loc[test_h1n1['race'] == 'White', 'race'] = 1\n",
        "test_h1n1.loc[test_h1n1['race'] == 'Black', 'race'] = 2\n",
        "test_h1n1.loc[test_h1n1['race'] == 'Other or Multiple', 'race'] = 3\n",
        "test_h1n1.loc[test_h1n1['race'] == 'Hispanic', 'race'] = 4\n",
        "\n",
        "\n",
        "## sex\n",
        "\n",
        "test_h1n1.loc[test_h1n1['sex'] == 'Female', 'sex'] = 1\n",
        "test_h1n1.loc[test_h1n1['sex'] == 'Male', 'sex'] = 2\n",
        "\n",
        "\n",
        "## hhs_geo_region\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(test_h1n1['hhs_geo_region'])\n",
        "\n",
        "print(le.classes_)\n",
        "test_h1n1['hhs_geo_region'] = le.transform(test_h1n1['hhs_geo_region'])\n",
        "\n",
        "\n",
        "## census_msa\n",
        "\n",
        "test_h1n1.loc[test_h1n1['census_msa'] == 'Non-MSA', 'census_msa'] = 1\n",
        "test_h1n1.loc[test_h1n1['census_msa'] == 'MSA, Not Principle  City', 'census_msa'] = 2\n",
        "test_h1n1.loc[test_h1n1['census_msa'] == 'MSA, Principle City', 'census_msa'] = 3\n"
      ],
      "metadata": {
        "id": "nu3Ur6OxGZpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 훈련을 위해 데이터타입 변환해주기\n",
        "\n",
        "test_h1n1['age_group'] = test_h1n1['age_group'].astype(np.int64)\n",
        "test_h1n1['race'] = test_h1n1['race'].astype(np.int64)\n",
        "test_h1n1['sex'] = test_h1n1['sex'].astype(np.int64)\n",
        "test_h1n1['census_msa'] = test_h1n1['census_msa'].astype(np.int64)"
      ],
      "metadata": {
        "id": "xRWDA-CYF4bA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_h1n1.head()"
      ],
      "metadata": {
        "id": "msFTiv6PMzYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_h1n1 = test_h1n1.drop(['respondent_id'], axis = 1)\n",
        "test_h1n1.head()"
      ],
      "metadata": {
        "id": "EhTGpwtdMf-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_h1n1 = model_h1n1.predict(test_h1n1)"
      ],
      "metadata": {
        "id": "2MfIJ75dCjEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_h1n1"
      ],
      "metadata": {
        "id": "Ss0liyXEN7I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_features['h1n1_vaccine'] = pred_h1n1"
      ],
      "metadata": {
        "id": "IGjOWuGZNjRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### seasonal 예측하기"
      ],
      "metadata": {
        "id": "MnsAPeasTGTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## seasonal 인코딩하기\n",
        "\n",
        "## age_group\n",
        "\n",
        "test_seasonal.loc[test_seasonal['age_group'] == '18 - 34 Years', 'age_group'] = 1\n",
        "test_seasonal.loc[test_seasonal['age_group'] == '35 - 44 Years', 'age_group'] = 2\n",
        "test_seasonal.loc[test_seasonal['age_group'] == '45 - 54 Years', 'age_group'] = 3\n",
        "test_seasonal.loc[test_seasonal['age_group'] == '55 - 64 Years', 'age_group'] = 4\n",
        "test_seasonal.loc[test_seasonal['age_group'] == '65+ Years', 'age_group'] = 5\n",
        "\n",
        "\n",
        "## race\n",
        "\n",
        "test_seasonal.loc[test_seasonal['race'] == 'White', 'race'] = 1\n",
        "test_seasonal.loc[test_seasonal['race'] == 'Black', 'race'] = 2\n",
        "test_seasonal.loc[test_seasonal['race'] == 'Other or Multiple', 'race'] = 3\n",
        "test_seasonal.loc[test_seasonal['race'] == 'Hispanic', 'race'] = 4\n",
        "\n",
        "\n",
        "## sex\n",
        "\n",
        "test_seasonal.loc[test_seasonal['sex'] == 'Female', 'sex'] = 1\n",
        "test_seasonal.loc[test_seasonal['sex'] == 'Male', 'sex'] = 2\n",
        "\n",
        "\n",
        "## hhs_geo_region\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(test_seasonal['hhs_geo_region'])\n",
        "\n",
        "print(le.classes_)\n",
        "test_seasonal['hhs_geo_region'] = le.transform(test_seasonal['hhs_geo_region'])\n",
        "\n",
        "\n",
        "## census_msa\n",
        "\n",
        "test_seasonal.loc[test_seasonal['census_msa'] == 'Non-MSA', 'census_msa'] = 1\n",
        "test_seasonal.loc[test_seasonal['census_msa'] == 'MSA, Not Principle  City', 'census_msa'] = 2\n",
        "test_seasonal.loc[test_seasonal['census_msa'] == 'MSA, Principle City', 'census_msa'] = 3\n"
      ],
      "metadata": {
        "id": "6lGX7jCjKQoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 훈련을 위해 데이터타입 변환해주기\n",
        "\n",
        "test_seasonal['age_group'] = test_seasonal['age_group'].astype(np.int64)\n",
        "test_seasonal['race'] = test_seasonal['race'].astype(np.int64)\n",
        "test_seasonal['sex'] = test_seasonal['sex'].astype(np.int64)\n",
        "test_seasonal['census_msa'] = test_seasonal['census_msa'].astype(np.int64)"
      ],
      "metadata": {
        "id": "HZPTecXVKeSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_seasonal.info()"
      ],
      "metadata": {
        "id": "_nlrBhDIKwmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_seasonal = test_seasonal.drop(['respondent_id'], axis = 1)\n",
        "test_seasonal.head()"
      ],
      "metadata": {
        "id": "rAtsGOciO7_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_seasonal = model_seasonal.predict(test_seasonal)"
      ],
      "metadata": {
        "id": "FlheLZ8qDhHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_seasonal"
      ],
      "metadata": {
        "id": "R0RhaQZ2Kzvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_features['seasonal_vaccine'] = pred_seasonal"
      ],
      "metadata": {
        "id": "dqTb3VMKPSVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_features.head()"
      ],
      "metadata": {
        "id": "4Sb-ecQLPWYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 제출 파일 만들기"
      ],
      "metadata": {
        "id": "-qzh87uuTkeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission = test_features[['respondent_id', 'h1n1_vaccine', 'seasonal_vaccine']]"
      ],
      "metadata": {
        "id": "mCuryyAwPYut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.head()"
      ],
      "metadata": {
        "id": "V6sSf7GRPhpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "## 셀 실행 후 왼쪽 파일 모양 클릭하고 'submission.csv' 파일을 다운받으면 됨."
      ],
      "metadata": {
        "id": "wJbx7X7dPi3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AIHOIRVI85Z4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}